{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi Task Learning \n",
    "\n",
    "The aim of multi task learning is to leverage two (or more) related tasks in the learning process with the hope that leanring one task aids perfromance in learning the other task(s) and thus improves predicitve power for at least one (ideally all) of the tasks. \n",
    "\n",
    "There are two distinct flavours of MTL: Hard parameter sharing and soft parameter sharing. In this project, the former is investigated.\n",
    "Hard paramter sharing invloves two tasks sharing a common network which splits into task specific paths (e.g. a series of convolutional layers with two paths of dense layers for two seperate tasks). \n",
    "\n",
    "This project uses the FASHION MNIST dataset to compare MTL against individual task networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The two tasks to be performed:\n",
    "\n",
    "- Task 1 - Clothing item 10 class classification (e.g. shoes, t-shirts etc) across 10 goups - $ y \\in \\mathbb{R^{10}} $\n",
    "- Task 2 - Clothing group three class classification - predicitng whether a viewed clothing image belongs to one of three groups - $ y \\in \\mathbb{R^{3}} $\n",
    "    - These groups are shoes (Sandal, Sneaker and Ankle Boot),  Gendered (Dress, Shirt and Bag) and Uni-Sex (T-shirt, Trouser, Pullover and Coat). \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.datasets.fashion_mnist as fashion_mnist\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST Fashion dataset\n",
    "\n",
    "def load_data(): \n",
    "    # train_X: (60000, 28, 28)\n",
    "    # train_y: (60000,)\n",
    "    # test_X: (10000, 28, 28)\n",
    "    # test_y: (10000,)\n",
    "    (train_X, train_y_1), (test_X, test_y_1) = fashion_mnist.load_data()\n",
    "    n_class_1 = 10\n",
    "    # map to new label\n",
    "    train_y_2 = list(0 if y in [5, 7, 9] else 1 if y in [3, 6, 8] else 2 for y in train_y_1)  \n",
    "    test_y_2 = list(0 if y in [5, 7, 9] else 1 if y in [3, 6, 8] else 2 for y in test_y_1)\n",
    "    n_class_2 = 3\n",
    "    # train_X: (60000, 28, 28, 1)\n",
    "    # test_X: (10000, 28, 28, 1)\n",
    "    # train_y: (60000, n_class = 10)\n",
    "    # test_y: (10000, n_class = 3)\n",
    "    train_X = np.expand_dims(train_X, axis=3)\n",
    "    test_X = np.expand_dims(test_X, axis=3)\n",
    "    train_y_1 = to_categorical(train_y_1, n_class_1)\n",
    "    test_y_1 = to_categorical(test_y_1, n_class_1)\n",
    "    train_y_2 = to_categorical(train_y_2, n_class_2)\n",
    "    test_y_2 = to_categorical(test_y_2, n_class_2)\n",
    "    return train_X, train_y_1, train_y_2, test_X, test_y_1, test_y_2\n",
    "\n",
    "\n",
    "x_train, y_train_1, y_train_2, x_test, y_test_1, y_test_2 = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task Specific Networks \n",
    "\n",
    "Below are two seperate network classes of identical structure (except the logits and pred layers), one for each of the two tasks.\n",
    "\n",
    "For the sake of convieience, both networks will use CNN filters - $[32, 64, 128]$. The kernel size will be 3 $\\times$ 3 and a stride of 1 for all convolutional layers. Maxpooling layers are used after the first and second convolutional layers. The maxpooling layers have a kernel size of two and a stride of 2. \n",
    "\n",
    "After the final convolution, the outputs are passed to dense layers $[3136, 1024, 100 , N]$ where $N$ is the number of outputs required (10 or 3). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task_1_NN():\n",
    "    def __init__(self, x_train, y_train_1,  output_dir, lr=0.001, nb_epochs=10, batch_size=50):\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.nb_images, self.edge, _, _ = x_train.shape\n",
    "        self.nb_iterations = self.nb_images // batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.x_train = x_train\n",
    "        self.y_train_1 = y_train_1\n",
    "        self.m = x_train.shape[0]\n",
    "        self.n_output_1 = y_train_1.shape[1]\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, (None, 28, 28, 1), \"X\")\n",
    "        self.y_1 = tf.placeholder(tf.float32, (None, self.n_output_1), \"y_1\")\n",
    "    \n",
    "    def create_model(self):            \n",
    "        with tf.variable_scope(\"Task_1\", reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            # Construct the network for task 1\n",
    "            self.hlayer1 = tf.layers.conv2d(self.X, filters=32, kernel_size=3, strides=(1, 1), activation=tf.nn.relu)\n",
    "            self.hlayer1 = tf.layers.max_pooling2d(self.hlayer1, pool_size=2, strides=2)\n",
    "            self.hlayer2 = tf.layers.conv2d(self.hlayer1, filters=64, kernel_size=3, strides=(1, 1), activation=tf.nn.relu)\n",
    "            self.hlayer2 = tf.layers.max_pooling2d(self.hlayer2, pool_size=2, strides=2)\n",
    "            self.hlayer3 = tf.layers.conv2d(self.hlayer2, filters=128, kernel_size=3, strides=(1, 1), activation=tf.nn.relu)\n",
    "            self.flatlayer = tf.layers.flatten(self.hlayer3)\n",
    "            self.hlayer4 = tf.layers.dense(self.flatlayer, 1024, tf.nn.relu)\n",
    "            self.hlayer5 = tf.layers.dense(self.hlayer4, 100, tf.nn.relu)\n",
    "            self.logits = tf.layers.dense(self.hlayer5, 10)\n",
    "            self.preds = tf.nn.softmax(self.logits)\n",
    "                \n",
    "    def compute_loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            # Calculate the loss \n",
    "            self.loss_task_1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_1, logits=self.logits))\n",
    "            self.loss_summ = tf.summary.scalar(\"softmax_loss\", self.loss_task_1) \n",
    "                \n",
    "                \n",
    "    def optimizer(self):\n",
    "        with tf.variable_scope('optimizer', reuse=tf.AUTO_REUSE):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5)\n",
    "            self.model_vars = tf.trainable_variables()\n",
    "            self.trainer = optimizer.minimize(self.loss_task_1, var_list=self.model_vars)\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Task_2_NN():\n",
    "    def __init__(self, x_train, y_train_2,  output_dir, lr=0.001, nb_epochs=10, batch_size=50):\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.nb_images, self.edge, _, _ = x_train.shape\n",
    "        self.nb_iterations = self.nb_images // batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.x_train = x_train\n",
    "        self.y_train_2 = y_train_2\n",
    "        self.m = x_train.shape[0]\n",
    "        self.n_output_2 = y_train_2.shape[1]\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, (None, 28, 28, 1), \"X\")\n",
    "        self.y_2 = tf.placeholder(tf.float32, (None, self.n_output_2), \"y_2\")\n",
    "    \n",
    "    def create_model(self):            \n",
    "        with tf.variable_scope(\"Task_2\", reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            # Construct network for task 2\n",
    "            self.hlayer1 = tf.layers.conv2d(self.X, filters=32, kernel_size=3, strides=(1, 1), activation=tf.nn.relu)\n",
    "            self.hlayer1 = tf.layers.max_pooling2d(self.hlayer1, pool_size=2, strides=2)\n",
    "            self.hlayer2 = tf.layers.conv2d(self.hlayer1, filters=64, kernel_size=3, strides=(1, 1), activation=tf.nn.relu)\n",
    "            self.hlayer2 = tf.layers.max_pooling2d(self.hlayer2, pool_size=2, strides=2)\n",
    "            self.hlayer3 = tf.layers.conv2d(self.hlayer2, filters=128, kernel_size=3, strides=(1, 1), activation=tf.nn.relu)\n",
    "            self.flatlayer = tf.layers.flatten(self.hlayer3)\n",
    "            self.hlayer4 = tf.layers.dense(self.flatlayer, 1024, tf.nn.relu)\n",
    "            self.hlayer5 = tf.layers.dense(self.hlayer4, 100, tf.nn.relu)\n",
    "            self.logits = tf.layers.dense(self.hlayer5, 3)\n",
    "            self.preds = tf.nn.softmax(self.logits)\n",
    "                \n",
    "    def compute_loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            # Calculate the loss \n",
    "            self.loss_task_2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_2, logits=self.logits))\n",
    "            self.loss_summ = tf.summary.scalar(\"softmax_loss\", self.loss_task_2) \n",
    "                \n",
    "                \n",
    "    def optimizer(self):\n",
    "        with tf.variable_scope('optimizer', reuse=tf.AUTO_REUSE):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5)\n",
    "            self.model_vars = tf.trainable_variables()\n",
    "            self.trainer = optimizer.minimize(self.loss_task_2, var_list=self.model_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate the model for task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists to hold loss and training accuracy\n",
    "train_acc_task_1 = []\n",
    "train_acc_task_2 = []\n",
    "\n",
    "test_acc_task_1 = []\n",
    "test_acc_task_2 = []\n",
    "\n",
    "loss_task_1 = []\n",
    "loss_task_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialise model for task 1\n",
    "model_1 = Task_1_NN(x_train, y_train_1, './Task1_logdir/', 0.001, 10, 200)            \n",
    "model_1.create_model()     \n",
    "\n",
    "model_1.compute_loss()\n",
    "model_1.optimizer()   \n",
    "\n",
    "model_1.optimizer()\n",
    "init = (tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "summary =tf.Summary()\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "writer = tf.summary.FileWriter(model_1.output_dir)\n",
    "writer.add_graph(sess.graph)\n",
    "if not os.path.exists(model_1.output_dir):\n",
    "    os.makedirs(model_1.output_dir)  \n",
    "\n",
    "# Train the model for 10 epochs\n",
    "\n",
    "for epoch in range(model_1.nb_epochs):\n",
    "    # Shuffle training examplars\n",
    "    randomize = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(randomize)\n",
    "    x_in = model_1.x_train[randomize,:]\n",
    "    y_in_1 = model_1.y_train_1[randomize,:]\n",
    "    \n",
    "    for i in range(model_1.nb_iterations):\n",
    "        \n",
    "        input_x_train = x_in[i*model_1.batch_size: (i+1)*model_1.batch_size]\n",
    "        input_y_train_1 = y_in_1[i*model_1.batch_size: (i+1)*model_1.batch_size]\n",
    "        _ , preds_1, loss_1, loss_summ = sess.run([model_1.trainer, model_1.preds,  model_1.loss_task_1, model_1.loss_summ], \n",
    "                                 feed_dict={model_1.X: input_x_train, \n",
    "                                            model_1.y_1: input_y_train_1})\n",
    "\n",
    "        y_preds_1 = np.argmax(preds_1, axis=1)\n",
    "        y_real_1 = np.argmax(input_y_train_1, axis=1)\n",
    "        acc_train_1 = np.mean((y_preds_1==y_real_1)*1)\n",
    "        # Record loss and train accuracy\n",
    "        train_acc_task_1.append(acc_train_1)\n",
    "        loss_task_1.append(loss_1)\n",
    "        #print('Epoch %d, Iteration %d, loss_1 %.3f,  batch accuracy_1 %.3f' %(epoch, i, loss_1,acc_train_1))\n",
    "        writer.add_summary(loss_summ, epoch * model_1.nb_iterations + i)\n",
    "    saver.save(sess, model_1.output_dir, global_step=epoch)\n",
    "    \n",
    "    #Testing accuracy for entire test set each epoch\n",
    "    preds = sess.run(model_1.preds, feed_dict={model_1.X: x_test})\n",
    "    y_preds = np.argmax(preds, axis=1)\n",
    "    y_real = np.argmax(y_test_1, axis=1)\n",
    "    acc_test = np.mean((y_preds==y_real)*1)\n",
    "    test_acc_task_1.append(acc_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the final test accuracy of the task 1 model\n",
    "\n",
    "batch_size_test = 20\n",
    "nb_test_points = x_test.shape[0] \n",
    "nb_iterations = nb_test_points//batch_size_test\n",
    "preds_1 = []\n",
    "for i in range(nb_iterations):\n",
    "    input_x_test = x_test[i*batch_size_test: (i+1)*batch_size_test]\n",
    "    preds_test_1 = sess.run(model_1.preds, \n",
    "                             feed_dict={model_1.X: input_x_test})\n",
    "    preds_1.append(np.argmax(preds_test_1, axis=1))\n",
    "    if np.mod(nb_test_points, batch_size_test) !=0:\n",
    "        input_x_test = x_test[i*batch_size_test: -1]\n",
    "        preds_test_1= sess.run(model_1.preds, \n",
    "                             feed_dict={model_1.X: input_x_test})\n",
    "        preds_1.append(np.argmax(preds_test_1, axis=1))\n",
    "all_preds_1 = np.concatenate(preds_1, axis =0)\n",
    "y_real_1 = np.argmax(y_test_1, axis=1)\n",
    "print(all_preds_1)\n",
    "print(y_real_1)\n",
    "acc_test_1 = np.mean((all_preds_1==y_real_1)*1)\n",
    "print('Test accuracy - task 1 achieved: %.3f' %acc_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate the model for task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Initialise model for task 2\n",
    "model_2 = Task_2_NN(x_train, y_train_2, './Task2_logdir/', 0.001, 10, 200)            \n",
    "model_2.create_model()     \n",
    "\n",
    "model_2.compute_loss()\n",
    "model_2.optimizer()   \n",
    "\n",
    "model_2.optimizer()\n",
    "init = (tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "summary =tf.Summary()\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "writer = tf.summary.FileWriter(model_2.output_dir)\n",
    "writer.add_graph(sess.graph)\n",
    "if not os.path.exists(model_2.output_dir):\n",
    "    os.makedirs(model_2.output_dir)\n",
    "    \n",
    "# Train the model for 10 epochs\n",
    "\n",
    "for epoch in range(model_2.nb_epochs):\n",
    "    \n",
    "    # Shuffle training examplars\n",
    "    randomize = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(randomize)\n",
    "    x_in = model_2.x_train[randomize,:]\n",
    "    y_in_2 = model_2.y_train_2[randomize,:]\n",
    "    \n",
    "    for i in range(model_2.nb_iterations):\n",
    "        input_x_train = x_in[i*model_2.batch_size: (i+1)*model_2.batch_size]\n",
    "        input_y_train_2 = y_in_2[i*model_2.batch_size: (i+1)*model_2.batch_size]\n",
    "        _ , preds_2, loss_2, loss_sum = sess.run([model_2.trainer, model_2.preds,  model_2.loss_task_2, model_2.loss_summ], \n",
    "                                 feed_dict={model_2.X: input_x_train, \n",
    "                                            model_2.y_2: input_y_train_2})\n",
    "        y_preds_2 = np.argmax(preds_2, axis=1)\n",
    "        y_real_2 = np.argmax(input_y_train_2, axis=1)\n",
    "        acc_train_2 = np.mean((y_preds_2==y_real_2)*1)\n",
    "        \n",
    "        #Record loss and train accuracy\n",
    "        train_acc_task_2.append(acc_train_2)\n",
    "        loss_task_2.append(loss_2)\n",
    "        #print('Epoch %d, Iteration %d, loss_2 %.3f, batch accuracy_2 %.3f' %(epoch, i, loss_2,acc_train_2))\n",
    "        writer.add_summary(loss_sum, epoch * model_2.nb_iterations + i)\n",
    "    saver.save(sess, model_2.output_dir, global_step=epoch)\n",
    "    \n",
    "    #Testing accuracy for entire test set each epoch\n",
    "    \n",
    "    preds = sess.run(model_2.preds, feed_dict={model_2.X: x_test})\n",
    "    y_preds = np.argmax(preds, axis=1)\n",
    "    y_real = np.argmax(y_test_2, axis=1)\n",
    "    acc_test = np.mean((y_preds==y_real)*1)\n",
    "    test_acc_task_2.append(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the final test accuracy of the task 2 model\n",
    "\n",
    "batch_size_test = 20\n",
    "nb_test_points = x_test.shape[0]\n",
    "nb_iterations = nb_test_points//batch_size_test\n",
    "preds_2 = []\n",
    "for i in range(nb_iterations):\n",
    "    input_x_test = x_test[i*batch_size_test: (i+1)*batch_size_test]\n",
    "    preds_test_2 = sess.run(model_2.preds, \n",
    "                             feed_dict={model_2.X: input_x_test})\n",
    "    preds_2.append(np.argmax(preds_test_2, axis=1))\n",
    "    if np.mod(nb_test_points, batch_size_test) !=0:\n",
    "        input_x_test = x_test[i*batch_size_test: -1]\n",
    "        preds_test_2= sess.run([model_2.preds], \n",
    "                             feed_dict={model_2.X: input_x_test})\n",
    "        preds_2.append(np.argmax(preds_test_2, axis=1))\n",
    "all_preds_2 = np.concatenate(preds_2, axis =0)\n",
    "y_real_2 = np.argmax(y_test_2, axis=1)\n",
    "acc_test_2 = np.mean((all_preds_2==y_real_2)*1)\n",
    "\n",
    "print('Test accuracy - task 2 achieved: %.3f' %acc_test_2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multi-Task Learning \n",
    "\n",
    "Here we build the multi-task learning network.\n",
    "\n",
    "Model inputs:\n",
    " - x_train, the training matrix\n",
    " - y_train_1, the fashion labels for task 1 (Fashion Item  classification)\n",
    " - y_train_2, the labels for task 2 (Fashion Group classification)\n",
    " - $\\lambda \\in [0,1]$ , lambda_, the loss weight for task 1 (1- $\\lambda$) is the loss weight for task 2\n",
    " - output_dir, the directory where model parameters and tensorbaord event files will be stored. \n",
    " - lr, the learning rate for ADAM\n",
    " - nb_epochs, the number of epochs to use\n",
    " - batch_size, the number of data points in each mini-batch\n",
    "\n",
    "The Multi Task Learning architecture is comprised of a shared convolutional backbone of three layers and a single shared fully connected layer with pooling between the first two pairs of convolutions.  The output of the shared fully connected layer is passed to two series of task specific dense layers, one for each of the two tasks. \n",
    "\n",
    "The architecture is as follows:\n",
    " - Shared Convolutional layers $[32, 64, 128]$ with max pooling after the first and second conv layers\n",
    "     - kernel size ($ 3 \\times 3$) for conv and ($2 \\times 2$) for max pool\n",
    "     - stride 1 for conv and 2 for max pooling\n",
    " - Flatten \n",
    " - Shared Dense Layer $[3136]$ - the outputs of which are passed to the two task dense layers\n",
    " - Task 1 Dense Layers $[1024, 100, 10]$ - 10 is the dimenson of the logits/preds\n",
    " - Task 2 Dense Layers $[1024, 100, 3]$ - 3 is the dimenson of the logits/preds \n",
    " - Task 1 Activation Layer - as earlier we use softmax\n",
    " - Task 2 Activation Layer  - as earlier we use softmax\n",
    " \n",
    "The total loss, which is a sum of the weighted losses for tasks 1 and 2 ($\\lambda * L_1 + (1-\\lambda) * L_2$) is passed to the optimiser. In the following experiments $\\lambda$ is set to 0.5 i.e. each loss contributes equally to the total loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MTL:\n",
    "    def __init__(self, x_train, y_train_1, y_train_2, lambda_, output_dir, lr=0.001, nb_epochs=10, batch_size=50):\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.lr = lr\n",
    "        self.batch_size = batch_size\n",
    "        self.nb_epochs = nb_epochs\n",
    "        self.nb_images, self.edge, _, _ = x_train.shape\n",
    "        self.nb_iterations = self.nb_images // batch_size\n",
    "        self.output_dir = output_dir\n",
    "        self.x_train = x_train\n",
    "        self.y_train_1 = y_train_1\n",
    "        self.y_train_2 = y_train_2\n",
    "        self.lambda_ = lambda_\n",
    "        \n",
    "        self.m = x_train.shape[0]\n",
    "        self.n_output_1 = y_train_1.shape[1]\n",
    "        self.n_output_2 = y_train_2.shape[1]\n",
    "        \n",
    "        self.X = tf.placeholder(tf.float32, (None, 28, 28, 1), \"X\")\n",
    "        self.y_1 = tf.placeholder(tf.float32, (None, self.n_output_1), \"y_1\")\n",
    "        self.y_2 = tf.placeholder(tf.float32, (None, self.n_output_2), \"y_2\")\n",
    "\n",
    "    \n",
    "    def create_model(self):            \n",
    "        with tf.variable_scope(\"MTL\", reuse=tf.AUTO_REUSE):\n",
    "            \n",
    "            # Constructed Multi Task Learning Network\n",
    "            \n",
    "            #Shared part network\n",
    "            self.hlayer1 = tf.layers.conv2d(self.X, filters=32, kernel_size=3, strides=(1, 1), activation=tf.nn.relu)\n",
    "            self.hlayer1 = tf.layers.max_pooling2d(self.hlayer1, pool_size=2, strides=2)\n",
    "            self.hlayer2 = tf.layers.conv2d(self.hlayer1, filters=64, kernel_size=3, strides=(1, 1), activation=tf.nn.relu)\n",
    "            self.hlayer2 = tf.layers.max_pooling2d(self.hlayer2, pool_size=2, strides=2)\n",
    "            self.hlayer3 = tf.layers.conv2d(self.hlayer2, filters=128, kernel_size=3, strides=(1, 1), activation=tf.nn.relu)\n",
    "            self.flatlayer = tf.layers.flatten(self.hlayer3)\n",
    "            self.hlayer4 = tf.layers.dense(self.flatlayer, 1024, tf.nn.relu)\n",
    "            \n",
    "            #Task 1 specific split\n",
    "            self.hlayer_task_1 = tf.layers.dense(self.hlayer4, 100, tf.nn.relu)\n",
    "            self.logits_1 = tf.layers.dense(self.hlayer_task_1, 10)\n",
    "            self.pred_1 = tf.nn.softmax(self.logits_1)\n",
    "            \n",
    "            #Task 2 specific split\n",
    "            self.hlayer_task_2 = tf.layers.dense(self.hlayer4, 100, tf.nn.relu)\n",
    "            self.logits_2 = tf.layers.dense(self.hlayer_task_2, 3)\n",
    "            self.pred_2 = tf.nn.softmax(self.logits_2)\n",
    "        \n",
    "    def compute_loss(self):\n",
    "        with tf.variable_scope('loss'):\n",
    "            # Compute the loss for each task and calculate weighted sum to give total loss\n",
    "            self.loss_task_1 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_1, logits=self.logits_1))\n",
    "\n",
    "            self.loss_task_2 = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=self.y_2, logits=self.logits_2))\n",
    "\n",
    "            self.loss_total = self.lambda_*self.loss_task_1 + (1-self.lambda_)*self.loss_task_2 \n",
    "            \n",
    "            self.loss_task_1_graph = tf.summary.scalar(\"softmax_loss_task_1\", self.loss_task_1) \n",
    "            self.loss_task_2_graph = tf.summary.scalar(\"softmax_loss_task_2\", self.loss_task_2)             \n",
    "            self.loss_sum = tf.summary.scalar(\"softmax_loss\", self.loss_total) \n",
    "            \n",
    "                \n",
    "                \n",
    "    def optimizer(self):\n",
    "        with tf.variable_scope('optimizer', reuse=tf.AUTO_REUSE):\n",
    "            optimizer = tf.train.AdamOptimizer(learning_rate=self.lr, beta1=0.5)\n",
    "            self.model_vars = tf.trainable_variables()\n",
    "            self.trainer = optimizer.minimize(self.loss_total, var_list=self.model_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise Multi Task Learning Model\n",
    "\n",
    "tf.reset_default_graph()\n",
    "model = MTL(x_train, y_train_1, y_train_2, 0.5, './MTL_logdir/', 0.001, 10, 200)             #       \n",
    "model.create_model()     \n",
    "\n",
    "model.compute_loss()\n",
    "model.optimizer()   \n",
    "\n",
    "model.optimizer()\n",
    "init = (tf.global_variables_initializer(),\n",
    "        tf.local_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "summary =tf.Summary()\n",
    "\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(init)\n",
    "writer = tf.summary.FileWriter(model.output_dir)\n",
    "writer.add_graph(sess.graph)\n",
    "if not os.path.exists(model.output_dir):\n",
    "    os.makedirs(model.output_dir) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Multi Task Learning model and evaluate on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to hold training and test accuracy and losses for both task 1 and 2\n",
    "train_acc_MTL_task_1 = []\n",
    "train_acc_MTL_task_2 = []\n",
    "\n",
    "test_acc_MTL_task_1 = []\n",
    "test_acc_MTL_task_2 = []\n",
    "\n",
    "loss_MTL_task_1 = []\n",
    "loss_MTL_task_2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Train Multi Task Learning model for 10 epochs \n",
    "\n",
    "for epoch in range(model.nb_epochs):\n",
    "    randomize = np.arange(x_train.shape[0])\n",
    "    np.random.shuffle(randomize)\n",
    "    x_in = model.x_train[randomize,:]\n",
    "    y_in_1 = model.y_train_1[randomize,:]\n",
    "    y_in_2 = model.y_train_2[randomize,:]\n",
    "    for i in range(model.nb_iterations):\n",
    "        input_x_train = x_in[i*model.batch_size: (i+1)*model.batch_size]\n",
    "        input_y_train_1 = y_in_1[i*model.batch_size: (i+1)*model.batch_size]\n",
    "        input_y_train_2 = y_in_2[i*model.batch_size: (i+1)*model.batch_size]\n",
    "        _ , preds_1, preds_2, loss_1, loss_2, loss_summ= sess.run([model.trainer, model.pred_1, model.pred_2, model.loss_task_1, model.loss_task_2, model.loss_sum], \n",
    "                                 feed_dict={model.X: input_x_train, \n",
    "                                            model.y_1: input_y_train_1,\n",
    "                                            model.y_2: input_y_train_2})\n",
    "\n",
    "        y_preds_1 = np.argmax(preds_1, axis=1)\n",
    "        y_preds_2 = np.argmax(preds_2, axis=1)\n",
    "        y_real_1 = np.argmax(input_y_train_1, axis=1)\n",
    "        y_real_2 = np.argmax(input_y_train_2, axis=1)\n",
    "        acc_train_1 = np.mean((y_preds_1==y_real_1)*1)\n",
    "        acc_train_2 = np.mean((y_preds_2==y_real_2)*1)\n",
    "        \n",
    "        #Record loss and train accuracy\n",
    "        train_acc_MTL_task_1.append(acc_train_1)\n",
    "        train_acc_MTL_task_2.append(acc_train_2)\n",
    "\n",
    "        loss_MTL_task_1.append(loss_1)\n",
    "        loss_MTL_task_2.append(loss_2)\n",
    "        #print('Epoch %d, Iteration %d, loss_1 %.3f, loss_2 %.3f, batch accuracy_1 %.3f, batch accuracy_2 %.3f' %(epoch, i, loss_1, loss_2, acc_train_1, acc_train_2))\n",
    "        writer.add_summary(loss_summ, epoch * model.nb_iterations + i)\n",
    "    saver.save(sess, model.output_dir, global_step=epoch)\n",
    "    \n",
    "    #Testing accuracy for entire test set each epoch\n",
    "    pred_1, pred_2 = sess.run([model.pred_1, model.pred_2], feed_dict={model.X: x_test})\n",
    "    \n",
    "    y_pred_1 = np.argmax(pred_1, axis=1)\n",
    "    y_real_1 = np.argmax(y_test_1, axis=1)\n",
    "    acc_test_1 = np.mean((y_pred_1==y_real_1)*1)\n",
    "    test_acc_MTL_task_1.append(acc_test_1)\n",
    "    \n",
    "    y_pred_2 = np.argmax(pred_2, axis=1)\n",
    "    y_real_2 = np.argmax(y_test_2, axis=1)\n",
    "    acc_test_2 = np.mean((y_pred_2==y_real_2)*1)\n",
    "    test_acc_MTL_task_2.append(acc_test_2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute test accuracy for final model\n",
    "\n",
    "batch_size_test = 20\n",
    "nb_test_points = x_test.shape[0] \n",
    "nb_iterations = nb_test_points//batch_size_test\n",
    "preds_1 = []\n",
    "preds_2 = []\n",
    "for i in range(nb_iterations):\n",
    "    input_x_test = x_test[i*batch_size_test: (i+1)*batch_size_test]\n",
    "    preds_test_1, preds_test_2 = sess.run([model.pred_1, model.pred_2], \n",
    "                             feed_dict={model.X: input_x_test})\n",
    "    preds_1.append(np.argmax(preds_test_1, axis=1))\n",
    "    preds_2.append(np.argmax(preds_test_2, axis=1))\n",
    "    if np.mod(nb_test_points, batch_size_test) !=0:\n",
    "        input_x_test = x_test[i*batch_size_test: -1]\n",
    "        preds_test_1, preds_test_2 = sess.run([model.pred_1, model.pred_2], \n",
    "                             feed_dict={model.X: input_x_test})\n",
    "        preds_1.append(np.argmax(preds_test_1, axis=1))\n",
    "        preds_2.append(np.argmax(preds_test_2, axis=1))\n",
    "all_preds_1 = np.concatenate(preds_1, axis =0)\n",
    "all_preds_2 = np.concatenate(preds_2, axis =0)\n",
    "y_real_1 = np.argmax(y_test_1, axis=1)\n",
    "y_real_2 = np.argmax(y_test_2, axis=1)\n",
    "acc_test_1 = np.mean((all_preds_1==y_real_1)*1)\n",
    "acc_test_2 = np.mean((all_preds_2==y_real_2)*1)\n",
    "print('Test accuracy - task 1 achieved: %.3f' %acc_test_1)\n",
    "print('Test accuracy - task 2 achieved: %.3f' %acc_test_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ploting the performance of Multi Task Learning vs Task Specific Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function for smoothing data\n",
    "def smooth(array, mov_avg_size, step_size):\n",
    "    \n",
    "    N = array.shape[0]\n",
    "    out = []\n",
    "    \n",
    "    cursor = 0\n",
    "    \n",
    "    while (cursor + mov_avg_size) <= N:\n",
    "        \n",
    "        out.append(sum(array[cursor:cursor+mov_avg_size])/mov_avg_size)\n",
    "        cursor += step_size\n",
    "        \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add plot of network performance \n",
    "#Iteration vs. Training Accuracy for Individual Tasks and MTL\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "\n",
    "train_acc_task_1_smooth = smooth(np.array(train_acc_task_1), 200, 1)\n",
    "train_acc_task_2_smooth = smooth(np.array(train_acc_task_2), 200, 1)\n",
    "train_acc_MTL_task_1_smooth = smooth(np.array(train_acc_MTL_task_1), 200, 1)\n",
    "train_acc_MTL_task_2_smooth = smooth(np.array(train_acc_MTL_task_2), 200, 1)\n",
    "\n",
    "ax.plot(train_acc_task_1_smooth,label='Task 1') \n",
    "ax.plot(train_acc_task_2_smooth,label='Task 2') \n",
    "ax.plot(train_acc_MTL_task_1_smooth,label='MTL Task 1') \n",
    "ax.plot(train_acc_MTL_task_2_smooth,label='MTL Task 2') \n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Training Accuracy')\n",
    "ax.set_title('Iteration vs. Training Accuracy for Individual Tasks and Multi-Task Learning')\n",
    "ax.legend()\n",
    "#plt.ylim(0.9,1.0)\n",
    "#plt.xlim(0,2800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add plot of network performance \n",
    "#Iteration vs. Training Accuracy for Individual Tasks and MTL\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "\n",
    "train_acc_task_1_smooth = smooth(np.array(train_acc_task_1), 200, 1)\n",
    "train_acc_task_2_smooth = smooth(np.array(train_acc_task_2), 200, 1)\n",
    "train_acc_MTL_task_1_smooth = smooth(np.array(train_acc_MTL_task_1), 200, 1)\n",
    "train_acc_MTL_task_2_smooth = smooth(np.array(train_acc_MTL_task_2), 200, 1)\n",
    "\n",
    "ax.plot(train_acc_task_1_smooth,label='Task 1') \n",
    "ax.plot(train_acc_task_2_smooth,label='Task 2') \n",
    "ax.plot(train_acc_MTL_task_1_smooth,label='MTL Task 1') \n",
    "ax.plot(train_acc_MTL_task_2_smooth,label='MTL Task 2') \n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Training Accuracy')\n",
    "ax.set_title('Iteration vs. Training Accuracy for Individual Tasks and Multi-Task Learning')\n",
    "ax.legend()\n",
    "#plt.ylim(0.9,1.0)\n",
    "#plt.xlim(0,2800)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add plot of network performance \n",
    "#Iteration vs. Network loss for Individual Tasks and MTL\n",
    "fig, ax = plt.subplots(figsize=(15,8))\n",
    "\n",
    "loss_task_1_smooth = smooth(np.array(loss_task_1), 200, 1)\n",
    "loss_task_2_smooth = smooth(np.array(loss_task_2), 200, 1)\n",
    "loss_MTL_task_1_smooth = smooth(np.array(loss_MTL_task_1), 200, 1)\n",
    "loss_MTL_task_2_smooth = smooth(np.array(loss_MTL_task_2), 200, 1)\n",
    "\n",
    "ax.plot(loss_task_1_smooth,label='Task 1') \n",
    "ax.plot(loss_task_2_smooth,label='Task 2') \n",
    "ax.plot(loss_MTL_task_1_smooth,label='MTL Task 1') \n",
    "ax.plot(loss_MTL_task_2_smooth,label='MTL Task 2') \n",
    "ax.set_xlabel('Iterations')\n",
    "ax.set_ylabel('Network Loss')\n",
    "ax.set_title('Iteration vs. Network Loss for Individual Tasks and Multi-Task Learning')\n",
    "ax.legend()\n",
    "#plt.ylim(0.9,1.0)\n",
    "plt.xlim(1)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
